First, install [detectron2](https://github.com/facebookresearch/detectron2/blob/main/INSTALL.md).
Then, download the [UniDet](https://github.com/xingyizhou/UniDet)'s
[weights](https://drive.google.com/file/d/110JSpmfNU__7T3IMSJwv0QSfLLo_AqtZ)
and [configurations](https://github.com/xingyizhou/UniDet/blob/master/configs/Partitioned_COI_RS101_2x.yaml).

## Counting, Spatail, Size Composition

Run the
[inference code](detection/UniDet-master/demo.py)
to generate the bounding boxes and save them as follows:
```bash
cd data_evaluate_LLM/eval_metrics/detection/UniDet-master
```
```python
python demo.py --config-file [configs] \
--input [path_to_images]/* --pkl_pth [path_save] \
--output [detected_images] --opts MODEL.WEIGHTS [model_weight]
```
Where:
- `--config-file`: use  configs/Partitioned_COI_RS101_2x.yaml config
- `--input`: Folder of images that need to be evaluated
- `--pkl_pth`: The path to the output .pkl file to save detected information
- `--output`: Path to the folder to save visually detected images
- `--opts`: Path to the weights downloaded above
eg.
```bash
python demo.py --config-file configs/Partitioned_COI_RS101_2x.yaml --input  
"GLIGEN/visual/counting_500_img/*" --pkl_pth "../../counting/counting_500_1499.pkl"  --output "detected" --opts MODEL.WEIGHTS "Partitioned_COI_RS101_2x.pth"
```


### Counting 
Run the 
[calc_counting_acc.py](counting/calc_counting_acc.py)
to calculate the counting accuracy, as follows:
```bash
cd data_evaluate_LLM/eval_metrics/counting
python calc_counting_acc.py [Input pkl path] [GT-csv] [Number of Iteration]
```
eg.
```bash
python calc_counting_acc.py 'Attend-and-Excite/counting_5000.pkl,Attend-and-Excite/counting_500_1499.pkl' ../../HRS/counting_prompts.csv 1
```

### Spatial composition
Run the 
[calc_spatial_relation_acc.py](compositions/calc_spatial_relation_acc.py)
to calculate the spatial composition accuracy, as follows:
```bash
cd data_evaluate_LLM/eval_metrics/compositions
```
```python
python calc_spatial_relation_acc.py [Input pkl path] [GT-csv] [Number of Iteration]
```
eg.
```python
python calc_spatial_relation_acc.py 'Attend-and-Excite/spatial_500.pkl,Attend-and-Excite/spatial_1000.pkl' ../../HRS/spatial_compositions_prompts.csv 1
```
### Size
Run the 
[calc_size_comp_acc.py](compositions/calc_size_comp_acc.py)
to calculate the size composition accuracy, as follows:
```bash
cd data_evaluate_LLM/eval_metrics/compositions
```
```python
python calc_size_comp_acc.py [Input pkl path] [GT-csv] [Number of Iteration]
```
eg.
```python
python calc_spatial_relation_acc.py 'Attend-and-Excite/size.pkl' ../../HRS/size_compositions_prompts.csv 1
```
Where:
- `Input pkl path`: This is the output file from running UniDet as shown in the above steps. If there are multiple .pkl files for one category, use a comma (",") to separate each file path. For example: 'counting_500.pkl,counting_1000.pkl'.

- `GT-csv`: This is the .csv file which can be generated by running the prompt generation code or downloaded directly from our published prompts.

- `Number of Iteration`: This is an integer number

## Color Composition:
We adopt [MaskDINO](https://arxiv.org/pdf/2206.02777.pdf) [CVPr 2023] for the instance segmentation.
Download the [model weight](https://github.com/IDEA-Research/detrex-storage/releases/download/maskdino-v0.1.0/maskdino_swinl_50ep_300q_hid2048_3sd1_panoptic_58.3pq.pth), the corresponding [config](colors/MaskDINO/configs/coco/instance-segmentation/swin/maskdino_R50_bs16_50ep_4s_dowsample1_2048.yaml) 
First, run the 
[inference code](colors/MaskDINO/demo/demo.py)
to predict the masks for each instance and save them as follows:
```bash
cd data_evaluate_LLM/eval_metrics/colors/MaskDINO/demo
python demo.py [Config File] [Input Images Directory] [Output Images Directory] [Model Weights]
```
Where:

- `Config File`: config of segmentation model
- `Input Images`: Directory: path to folder images need to evaluate
- `Output Images`: Directory: path to output of segmented mask
- `Model weight`: path ot the model weight downloaded above

For instance:
```

python demo.py --config-file 'T2I_benchmark/codes/eval_metrics/colors/MaskDINO/configs/coco/instance-segmentation/swin/maskdino_R50_bs16_50ep_4s_dowsample1_2048.yaml' \
--input 'Attend-and-Excite/colors/*' \
--output Attend-and-Excite/ \
--opts MODEL.WEIGHTS T2I_benchmark/weights/mask_dino/maskdino_swinl_50ep_300q_hid2048_3sd1_instance_maskenhanced_mask52.3ap_box59.pth
```

Then, run the 
[hue_based_color_classifier.py](colors/hue_based_color_classifier.py)
to calculate the color composition accuracy, as follows:
```python
cd data_evaluate_LLM/eval_metrics/colors/
python hue_based_color_classifier.py [Generated Masks Directory] [GT-csv] [T2I Model Output Directory]
```
Where:
- `Generated Masks Directory`: the Output Images Directory in previous running
- `GT-csv`: is the csv file which can be generated by running the prompt generation code or downloaded directly from our published prompts.
- `T2I Model Output Directory`:  path to folder images need to evaluate

For instance:
```bash
python hue_based_color_classifier.py  'MaskDINO/demo/Attend-and-Excite' '../../HRS/colors_composition_prompts.csv' 'Attend-and-Excite/colors'
```
